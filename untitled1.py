# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ujxZl1GgNpL6dzGSUpbBUP_uyCDL4ymT
"""

import pandas as pd

train_data = pd.read_csv('/content/train_data.txt', sep='\t', names=['id', 'title', 'genre'], skiprows=1)
test_data = pd.read_csv('/content/test_data.txt', sep='\t', names=['id', 'title'], skiprows=1)
test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\t', names=['id', 'genre'], skiprows=1)

print("Training Data Shape:", train_data.shape)
print("Test Data Shape:", test_data.shape)
print("Test Solution Shape:", test_solution.shape)
print("\nTraining Data Sample:\n", train_data.head())
print("\nUnique Genres:\n", train_data['genre'].unique())
print("\nMissing Values in Train:\n", train_data.isnull().sum())

import pandas as pd
import re
train_data = pd.read_csv('/content/train_data.txt', sep='\t', names=['id', 'title', 'genre'], skiprows=1)
test_data = pd.read_csv('/content/test_data.txt', sep='\t', names=['id', 'title'], skiprows=1)
test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\t', names=['id', 'genre'], skiprows=1)

def clean_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        return text.strip()
    return ''
train_data['cleaned_text'] = train_data['title'].apply(clean_text)

print("Sample of cleaned text:\n", train_data['cleaned_text'].head(10))
print("\nEmpty cleaned text count:", (train_data['cleaned_text'] == '').sum())
print("\nNon-empty cleaned text sample:\n", train_data[train_data['cleaned_text'] != ''][['title', 'cleaned_text']].head())

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
try:
    X_train = vectorizer.fit_transform(train_data['cleaned_text'])
    print("TF-IDF Vocabulary size:", len(vectorizer.get_feature_names_out()))
except ValueError as e:
    print("Error:", e)

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer

train_data = pd.read_csv('/content/train_data.txt', sep='\t', names=['id', 'title', 'genre'], skiprows=1)
test_data = pd.read_csv('/content/test_data.txt', sep='\t', names=['id', 'title'], skiprows=1)
test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\t', names=['id', 'genre'], skiprows=1)

def extract_info(row):
    parts = row['id'].split(' ::: ')
    if len(parts) > 2:
        row['title'] = parts[1]
        row['genre'] = parts[2]
    return row

train_data = train_data.apply(extract_info, axis=1)
test_data = test_data.apply(extract_info, axis=1)
train_data = train_data.drop('id', axis=1).dropna(subset=['title', 'genre'])
test_data = test_data.drop('id', axis=1).dropna(subset=['title'])

def clean_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        return text.strip()
    return ''

train_data['cleaned_text'] = train_data['title'].apply(clean_text)
test_data['cleaned_text'] = test_data['title'].apply(clean_text)

print("Empty cleaned text count:", (train_data['cleaned_text'] == '').sum())
print("Non-empty cleaned text sample:\n", train_data[train_data['cleaned_text'] != ''][['title', 'cleaned_text']].head())

vectorizer = TfidfVectorizer(max_features=5000)
try:
    X_train = vectorizer.fit_transform(train_data['cleaned_text'])
    X_test = vectorizer.transform(test_data['cleaned_text'])
    print("TF-IDF Vocabulary size:", len(vectorizer.get_feature_names_out()))
except ValueError as e:
    print("Error:", e)

if 'X_train' in locals() and X_train.shape[1] > 0:
    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    X_train = vectorizer.fit_transform(train_data['cleaned_text'])
    X_test = vectorizer.transform(test_data['cleaned_text'])
    print("TF-IDF Vocabulary size (with stop words):", len(vectorizer.get_feature_names_out()))
else:
    print("Could not create TF-IDF vectors. Check data and cleaning process.")

y_train = train_data['genre']
y_test = test_solution['genre']

import pandas as pd
import numpy as np

try:
    train_data = pd.read_csv('/content/train_data.txt', sep='\t', names=['id', 'title', 'genre'], skiprows=1)
    test_data = pd.read_csv('/content/test_data.txt', sep='\t', names=['id', 'title'], skiprows=1)
    test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\t', names=['id', 'genre'], skiprows=1)
except pd.errors.ParserError:
    print("Error loading files. Please check the delimiter or file structure.")
    print("train_data.txt sample:")
    !head /content/train_data.txt
    print("\ntest_data.txt sample:")
    !head /content/test_data.txt
    print("\ntest_data_solution.txt sample:")
    !head /content/test_data_solution.txt
    raise
print("NaN values in train_data:\n", train_data.isnull().sum())
print("NaN values in test_data:\n", test_data.isnull().sum())
print("NaN values in test_solution:\n", test_solution.isnull().sum())
print("\nTrain Data Sample:\n", train_data.head())
print("\nTest Data Sample:\n", test_data.head())
print("\nTest Solution Sample:\n", test_solution.head())

train_data = train_data.dropna(subset=['genre'])
test_solution = test_solution.dropna(subset=['genre'])

if len(test_data) != len(test_solution):
    print(f"Warning: test_data ({len(test_data)}) and test_solution ({len(test_solution)}) have different lengths.")
    test_data = test_data[test_data['id'].isin(test_solution['id'])]
    test_solution = test_solution[test_solution['id'].isin(test_data['id'])]
    print(f"After alignment: test_data ({len(test_data)}), test_solution ({len(test_solution)})")

print("\nNaN values in train_data after cleaning:\n", train_data.isnull().sum())
print("NaN values in test_solution after cleaning:\n", test_solution.isnull().sum())

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Check if X_train and X_test are defined and non-empty
if 'X_train' in locals() and 'X_test' in locals() and X_train.shape[0] > 0 and X_test.shape[0] > 0:

    # Create Logistic Regression model with One-vs-Rest strategy
    model = LogisticRegression(
        multi_class='ovr',     # One-vs-Rest classification
        solver='saga',         # Solver suitable for large-scale and sparse data
        max_iter=1000,         # Maximum number of iterations
        n_jobs=-1,             # Use all CPU cores
        verbose=1              # Display progress output
    )

    print("Training Logistic Regression OVR model...")
    model.fit(X_train, y_train)

    print("Predicting...")
    y_pred = model.predict(X_test)

    print("\nEvaluation:")
    try:
        print("Test Accuracy:", accuracy_score(y_test, y_pred))
        print("\nClassification Report:\n", classification_report(y_test, y_pred))
    except ValueError as e:
        print("Evaluation error:", e)
        print("Sample y_test:\n", y_test[:10])
        print("Sample y_pred:\n", y_pred[:10])

else:
    print("Cannot train model: X_train or X_test is empty or not defined.")

import joblib
from google.colab import files
joblib.dump(model, 'movie_genre_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

streamlit_code = """
import streamlit as st
import joblib
import re
import numpy as np

model = joblib.load('movie_genre_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\\w\\s]', '', text)
    return text.strip()

st.title("Movie Genre Classifier")
st.write("Enter a movie description or title to predict its genre.")

user_input = st.text_area("Movie Description/Title", "")
if st.button("Predict Genre"):
    if user_input:
        cleaned_input = clean_text(user_input)
        X_input = vectorizer.transform([cleaned_input]).toarray()
        prediction = model.predict(X_input)[0]
        st.success(f"Predicted Genre: {prediction}")
    else:
        st.error("Please enter a movie description or title.")
"""

with open('app.py', 'w') as f:
    f.write(streamlit_code)

files.download('movie_genre_model.pkl')
files.download('tfidf_vectorizer.pkl')
files.download('app.py')

import joblib

joblib.dump(model, 'movie_genre_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

streamlit_code = """
import streamlit as st
import joblib
import re
import numpy as np

# Define LogisticRegressionOVR class
class LogisticRegressionOVR:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.classes = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def fit(self, X, y):
        self.classes = np.unique(y)
        n_samples, n_features = X.shape
        self.weights = np.zeros((len(self.classes), n_features))
        self.bias = np.zeros(len(self.classes))
        for i, cls in enumerate(self.classes):
            y_binary = np.where(y == cls, 1, 0)
            w = np.zeros(n_features)
            b = 0
            for _ in range(self.n_iterations):
                linear_model = np.dot(X, w) + b
                y_pred = self.sigmoid(linear_model)
                dw = (1/n_samples) * np.dot(X.T, (y_pred - y_binary))
                db = (1/n_samples) * np.sum(y_pred - y_binary)
                w -= self.learning_rate * dw
                b -= self.learning_rate * db
            self.weights[i] = w
            self.bias[i] = b

    def predict_proba(self, X):
        probs = np.zeros((X.shape[0], len(self.classes)))
        for i in range(len(self.classes)):
            probs[:, i] = self.sigmoid(np.dot(X, self.weights[i]) + self.bias[i])
        probs /= probs.sum(axis=1, keepdims=True)
        return probs

    def predict(self, X):
        probs = self.predict_proba(X)
        return self.classes[np.argmax(probs, axis=1)]

# Load model and vectorizer
model = joblib.load('movie_genre_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\\w\\s]', '', text)
    return text.strip()

st.title("Movie Genre Classifier")
st.write("Enter a movie description to predict its genre.")

user_input = st.text_area("Movie Description", "")
if st.button("Predict Genre"):
    if user_input:
        cleaned_input = clean_text(user_input)
        X_input = vectorizer.transform([cleaned_input]).toarray()
        prediction = model.predict(X_input)[0]
        st.success(f"Predicted Genre: {prediction}")
    else:
        st.error("Please enter a movie description.")
"""

with open('app.py', 'w') as f:
    f.write(streamlit_code)

!pip install streamlit pyngrok

# Install required packages
!pip install streamlit pyngrok --quiet

# Set your ngrok authtoken (only needs to be done once per session)
!ngrok config add-authtoken 2tTHGHJLW3emePMG0ArjsFQxl2Q_3fXqm2GiGCT7FDg2NPkUq

import os
import time
from pyngrok import ngrok

# Write the Streamlit app code to app.py
app_code = """
import streamlit as st

st.set_page_config(page_title="Drama Explorer", layout="centered")

st.title("ðŸŽ­ Drama Explorer")
st.write("Welcome! Explore emotional and dramatic scenes across genres.")

genre = st.selectbox("Choose a drama sub-genre:", ["Romantic", "Historical", "Crime", "Psychological"])
st.write(f"You selected **{genre}** drama.")

if genre == "Romantic":
    st.image("https://i.imgur.com/w1UJtvF.jpg", caption="Classic Romantic Drama")
    st.write("A tale of love, passion, and heartache.")
elif genre == "Historical":
    st.image("https://i.imgur.com/yGSKqTc.jpg", caption="Epic Historical Scene")
    st.write("A journey through time and trials.")
elif genre == "Crime":
    st.image("https://i.imgur.com/DbvIFvA.jpg", caption="Dark Crime Drama")
    st.write("Mystery, betrayal, and justice.")
else:
    st.image("https://i.imgur.com/oXzJcTn.jpg", caption="Mind-Bending Psychological Drama")
    st.write("Twists, turns, and the depths of the human mind.")
"""

with open("app.py", "w") as f:
    f.write(app_code)

# Kill any previous ngrok tunnels
ngrok.kill()

# Start Streamlit app in the background
os.system("streamlit run app.py &")

# Wait a few seconds to let Streamlit start
time.sleep(5)

# Open ngrok tunnel to Streamlit default port 8501
public_url = ngrok.connect("http://localhost:8501")

print("ðŸŽ¬ Your drama Streamlit app is live at:", public_url)